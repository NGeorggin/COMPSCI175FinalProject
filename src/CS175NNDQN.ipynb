{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NGeorggin/COMPSCI175FinalProject/blob/main/COMPSCI_175_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiB6hs7-YXKq"
      },
      "source": [
        "#COMPSCI 175 Final Project. Project Group 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSCe5Br6fQvb"
      },
      "source": [
        "##Introduction. TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQROmqL20xIj"
      },
      "source": [
        "##Libraries and Imports TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw00hMcIf3TD"
      },
      "source": [
        "Import the necessary RLCard library agents and functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgdEaymNYOWv",
        "outputId": "4d10cffd-8acf-49ee-ffa4-74d7b60a4098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rlcard in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from rlcard) (1.26.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from rlcard) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rlcard\n",
        "import rlcard\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import (\n",
        "    get_device,\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    reorganize,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "\n",
        "# TODO Cite HW2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import rlcard\n",
        "from rlcard.agents import (\n",
        "    CFRAgent,\n",
        "    RandomAgent,\n",
        ")\n",
        "from rlcard.utils import (\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import csv\n",
        "\n",
        "import rlcard\n",
        "from rlcard.envs import Env\n",
        "from rlcard.games.nolimitholdem import Game\n",
        "from rlcard.games.nolimitholdem.round import Action\n",
        "\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from rlcard.games.limitholdem import Game\n",
        "from rlcard.games.limitholdem import PlayerStatus\n",
        "\n",
        "from rlcard.games.nolimitholdem import Dealer\n",
        "from rlcard.games.nolimitholdem import Player\n",
        "from rlcard.games.nolimitholdem import Judger\n",
        "from rlcard.games.nolimitholdem import Round, Action\n",
        "\n",
        "\n",
        "\n",
        "from rlcard.games.base import Card\n",
        "\n",
        "\n",
        "\n",
        "from rlcard.games.limitholdem.utils import compare_hands\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from rlcard.games.nolimitholdem.dealer import NolimitholdemDealer\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "from rlcard.games.limitholdem import PlayerStatus\n",
        "\n",
        "###\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import rlcard\n",
        "from rlcard.agents import (\n",
        "    CFRAgent,\n",
        "    RandomAgent,\n",
        ")\n",
        "from rlcard.utils import (\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import (\n",
        "    get_device,\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    reorganize,\n",
        "    Logger,\n",
        "    plot_curve,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Cb-0esfQgV"
      },
      "source": [
        "##Neural Network Construction (For Guessing Opponent's Hand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zth9EI1TiNvT"
      },
      "outputs": [],
      "source": [
        "class NNPolicy(nn.Module): # TODO Cite HW2\n",
        "    def __init__(self, hidden):\n",
        "        super(NNPolicy, self).__init__()\n",
        "\n",
        "        # Input.\n",
        "        # Community Cards (5 input nodes), Opponent's Money they've Bet (1 input node), Opponent's Last Move (1 input node).\n",
        "        # 7 input nodes in total.\n",
        "\n",
        "        self.fc1 = nn.Conv1d(1, hidden, 1)\n",
        "        self.fc2 = nn.Conv1d(hidden, hidden, 1)\n",
        "        self.fc3 = nn.Conv1d(hidden, 10, 7)\n",
        "\n",
        "        # Output.\n",
        "        # Array of 10 probabilities that the opponent has a particular hand.\n",
        "        # 10 possible hands are: high card, pair, two pair, three of a kind, straight, flush, full house, four of a kind, straight flush, royal flush.\n",
        "\n",
        "\n",
        "    def probOpponentHand(self, communityCards, oppMoney, oppLastMove, device):\n",
        "\n",
        "        while len(communityCards) < 5:\n",
        "          communityCards.append(-1) # Denotes a card slot that has not been revealed yet\n",
        "\n",
        "        x = communityCards[:]\n",
        "        x.append(oppMoney)\n",
        "        x.append(oppLastMove)\n",
        "\n",
        "\n",
        "        x = torch.tensor(x).float().unsqueeze(0)\n",
        "\n",
        "\n",
        "        # TODO is this how you want it?\n",
        "        x = self.fc1(x) # Applies Layer 1 to the values\n",
        "        x = F.relu(x) # Applies ReLU filter to the values\n",
        "        x = self.fc2(x) # Applies Layer 2 to the values\n",
        "        x = F.relu(x) # Applies ReLU filter to the values\n",
        "        x = self.fc3(x) # Applies Layer 3 to the values\n",
        "\n",
        "        final = []\n",
        "\n",
        "        for e in x:\n",
        "          final.append(e)\n",
        "\n",
        "        final = torch.tensor(final).float().unsqueeze(0).to(device)\n",
        "\n",
        "        # We output the softmax\n",
        "        return F.softmax(final, dim=1)\n",
        "\n",
        "    def getAction(self, communityCards, oppMoney, oppLastMove, device):\n",
        "        probs = self.probOpponentHand(communityCards, oppMoney, oppLastMove, device)#.cpu()\n",
        "        m = Categorical(probs)\n",
        "        # print(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action) # Action is some integer E [0, 9]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjYPWBEd_RbA"
      },
      "source": [
        "##Reinforcement Algorithm for the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QE7GikPmXMBs"
      },
      "outputs": [],
      "source": [
        "def cardSTOI(listCards):\n",
        "    \"\"\"\n",
        "    TODO Cite\n",
        "    https://rlcard.org/games.html#limit-texas-hold-em\n",
        "    \"\"\"\n",
        "\n",
        "    card2index = dict()\n",
        "    num = 0\n",
        "    for s in ['S', 'H', 'D', 'C']:\n",
        "      for r in ['A','2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q','K']:\n",
        "        card2index[s+r] = num\n",
        "        num += 1\n",
        "\n",
        "\n",
        "    newList = []\n",
        "    for c in listCards:\n",
        "      newList.append(card2index[c])\n",
        "    return newList\n",
        "\n",
        "def evaluateHandType(communityCards, twoCards):\n",
        "    \"\"\"\n",
        "    Hands labeled ...\n",
        "      0. High Card\n",
        "      1. Pair\n",
        "      2. Two Pair\n",
        "      3. Three Of A Kind\n",
        "      4. Straight\n",
        "      5. Flush\n",
        "      6. Full House\n",
        "      7. Four Of A Kind\n",
        "      8. Straight Flush\n",
        "      9. Royal Flush\n",
        "    \"\"\"\n",
        "\n",
        "    bigCardList = communityCards[:] + twoCards[:]\n",
        "\n",
        "    # print(bigCardList)\n",
        "\n",
        "    if len(bigCardList) < 5:\n",
        "        return -1\n",
        "\n",
        "    suits = {'S':0 , 'C':0 , 'H':0 , 'D':0}\n",
        "    ranks = {'2':0, '3':0, '4':0, '5':0, '6':0, '7':0, '8':0, '9':0, 'T':0, 'J':0, 'Q':0,'K':0,'A':0}\n",
        "\n",
        "    for c in bigCardList:\n",
        "        suits[c[0]] += 1\n",
        "        ranks[c[1]] += 1\n",
        "\n",
        "    stringOfRanks = \"\"\n",
        "    for v in list(ranks.values()):\n",
        "        if v == 0:\n",
        "          stringOfRanks += \"0\"\n",
        "        else:\n",
        "          stringOfRanks += \"1\"\n",
        "\n",
        "\n",
        "    if \"11111\" in stringOfRanks and (5 in list(suits.values()) or 6 in list(suits.values()) or 7 in list(suits.values())) and stringOfRanks.endswith(\"11111\"): ## Royal Flush\n",
        "        return 9\n",
        "    elif \"11111\" in stringOfRanks and (5 in list(suits.values()) or 6 in list(suits.values()) or 7 in list(suits.values())): ## Straight Flush\n",
        "        return 8\n",
        "    elif 4 in list(ranks.values()): ## Four of a Kind\n",
        "        return 7\n",
        "    elif (3 in list(ranks.values()) and 2 in list(ranks.values())) or list(ranks.values()).count(3) >= 2:## Full House\n",
        "        return 6\n",
        "    elif 5 in list(suits.values()) or 6 in list(suits.values()) or 7 in list(suits.values()): ## Flush\n",
        "        return 5\n",
        "    elif \"11111\" in stringOfRanks: ## Straight\n",
        "        return 4\n",
        "    elif 3 in list(ranks.values()): ## Three Of A Kind\n",
        "        return 3\n",
        "    elif list(ranks.values()).count(2) >= 2: ## Two Pair\n",
        "        return 2\n",
        "    elif 2 in list(ranks.values()): ## Pair\n",
        "        return 1\n",
        "    else: ## High Card\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNhAXxgaT2WN"
      },
      "source": [
        "## Combining Implementation of both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zApR06VkM78F"
      },
      "outputs": [],
      "source": [
        "def NNAffectPayoff(policy, optimizer, trajectories, payoffs, device):\n",
        "\n",
        "    agentHand = trajectories[0][-1]['raw_obs']['hand']\n",
        "    communityCards = trajectories[0][-1]['raw_obs']['public_cards']\n",
        "    __, oppMoney = trajectories[0][-1]['raw_obs']['all_chips']\n",
        "    oppHandActual = trajectories[-1][-1]['raw_obs']['hand']\n",
        "\n",
        "    oppLastMove = -1\n",
        "    ## TODO find opp last move\n",
        "\n",
        "    DQNExistingReward = abs(payoffs[0])\n",
        "    rewardAdjustment = 0.1 * DQNExistingReward\n",
        "\n",
        "    oppHandType, oppLogProb = policy.getAction(cardSTOI(communityCards), oppMoney, oppLastMove, device)\n",
        "\n",
        "\n",
        "    agentHandType = evaluateHandType(communityCards, agentHand)\n",
        "\n",
        "    if agentHandType == oppHandType or oppHandType == -1 or agentHandType == -1:\n",
        "        return payoffs\n",
        "\n",
        "    elif agentHandType > oppHandType:\n",
        "        payoffs[0] += rewardAdjustment\n",
        "        payoffs[1] -= rewardAdjustment\n",
        "\n",
        "    elif agentHandType < oppHandType:\n",
        "        payoffs[0] -= rewardAdjustment\n",
        "        payoffs[1] += rewardAdjustment\n",
        "\n",
        "\n",
        "    nnReturnProbs = policy.probOpponentHand(cardSTOI(communityCards), oppMoney, oppLastMove, device)#.cpu()\n",
        "\n",
        "    # print(oppHandType)\n",
        "    # print(len(list(nnReturnProbs)[0]))\n",
        "    rewardArray = [0] * len(list(nnReturnProbs)[0])\n",
        "    rewardArray[oppHandType] = 2 * rewardAdjustment\n",
        "\n",
        "    # print(nnReturnProbs)\n",
        "\n",
        "    # if oppHandActual == oppHandType:\n",
        "    #     nnReturnProbs[oppHandType] = 2*nnReturnProbs[oppHandType]\n",
        "\n",
        "    policyLoss = []\n",
        "    for i in range(len(nnReturnProbs)):\n",
        "        policyLoss.append(1 * (1-nnReturnProbs[0][i]) * rewardArray[i])\n",
        "    policyLoss = torch.tensor(policyLoss, requires_grad=True).sum() # TODO Cite https://discuss.pytorch.org/t/runtimeerror-element-0-of-variables-does-not-require-grad-and-does-not-have-a-grad-fn/11074\n",
        "\n",
        "\n",
        "    # print(f\"Policy Loss: {policyLoss}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policyLoss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    return payoffs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDQquMdJifgO"
      },
      "source": [
        "##DQN TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gllntIKxihSb"
      },
      "outputs": [],
      "source": [
        "def DQNtrain(args):\n",
        "\n",
        "    # Check whether gpu is available\n",
        "    device = get_device()\n",
        "\n",
        "    # Seed numpy, torch, random\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # Make the environment with seed\n",
        "    env = rlcard.make(\n",
        "        args.env,\n",
        "        config={\n",
        "            'seed': args.seed,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize the agent and use random agents as opponents\n",
        "    from rlcard.agents import DQNAgent\n",
        "    agent = DQNAgent(\n",
        "        num_actions=env.num_actions,\n",
        "        state_shape=env.state_shape[0],\n",
        "        mlp_layers=[64,128],\n",
        "        device=device,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    games_won = 0\n",
        "    csv_path2 = 'experiments/no_limit_holdem_dqn_result/result.csv'\n",
        "    with open(csv_path2, 'a') as f:\n",
        "        csv_writer = csv.writer(f)\n",
        "        csv_writer.writerow(['episode', 'reward'])\n",
        "\n",
        "    agents = [agent]\n",
        "    for _ in range(1, env.num_players):\n",
        "        agents.append(RandomAgent(num_actions=env.num_actions))\n",
        "    env.set_agents(agents)\n",
        "\n",
        "    # agent = torch.load(args.save_path) ## TODO put back\n",
        "\n",
        "    # Start training\n",
        "\n",
        "    nn = NNPolicy(256)\n",
        "    optimizer = optim.Adam(nn.parameters(), lr=0.001)\n",
        "\n",
        "    with Logger(args.log_dir) as logger:\n",
        "        for episode in range(args.num_episodes):\n",
        "\n",
        "            # Generate data from the environment\n",
        "            trajectories, payoffs = env.run(is_training=True)\n",
        "\n",
        "            ## REINFORCE\n",
        "\n",
        "            payoffs = NNAffectPayoff(nn, optimizer, trajectories, payoffs, device)\n",
        "\n",
        "            # Reorganaize the data to be state, action, reward, next_state, done\n",
        "            trajectories = reorganize(trajectories, payoffs)\n",
        "\n",
        "\n",
        "            # Feed transitions into agent memory, and train the agent\n",
        "            # Here, we assume that DQN always plays the first position\n",
        "            # and the other players play randomly (if any)\n",
        "            for ts in trajectories[0]:\n",
        "                agent.feed(ts)\n",
        "\n",
        "            # Evaluate the performance. Play with random agents.\n",
        "            if episode % args.evaluate_every == 0:\n",
        "                logger.log_performance(\n",
        "                    episode,\n",
        "                    tournament(\n",
        "                        env,\n",
        "                        args.num_eval_games,\n",
        "                    )[0]\n",
        "                )\n",
        "                with open(csv_path2, 'a') as f:\n",
        "                    csv_writer = csv.writer(f)\n",
        "                    if episode == 0:\n",
        "                        win_rate = 0\n",
        "                    else:\n",
        "                        win_rate = games_won / episode\n",
        "                    csv_writer.writerow([episode, win_rate])\n",
        "\n",
        "        # Get the paths\n",
        "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
        "        fig_path2 = 'experiments/no_limit_holdem_dqn_result/fig2.png'\n",
        "        plot_curve(csv_path2, fig_path2, 'win_rate')\n",
        "\n",
        "    # Plot the learning curve\n",
        "    plot_curve(csv_path, fig_path, args.algorithm)\n",
        "\n",
        "    # Save model ## TODO put back\n",
        "    save_path = os.path.join(args.log_dir, 'model.pth')\n",
        "    torch.save(agent, save_path)\n",
        "    #agent.save_checkpoint('/content/experiments/no_limit_holdem_dqn_result/')\n",
        "    print('Model saved in', save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GUfh8DAhihG6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b27ee7d9-16b3-4ae7-f167-d0797fcdc1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Running on the CPU\n",
            "\n",
            "----------------------------------------\n",
            "  episode      |  0\n",
            "  reward       |  -7.07894\n",
            "----------------------------------------\n",
            "INFO - Step 100, rl-loss: 888.2239990234375\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 165, rl-loss: 551.539306640625\n",
            "----------------------------------------\n",
            "  episode      |  100\n",
            "  reward       |  -0.26984\n",
            "----------------------------------------\n",
            "INFO - Step 316, rl-loss: 1423.94384765625\n",
            "----------------------------------------\n",
            "  episode      |  200\n",
            "  reward       |  1.40008\n",
            "----------------------------------------\n",
            "INFO - Step 482, rl-loss: 2073.5146484375\n",
            "----------------------------------------\n",
            "  episode      |  300\n",
            "  reward       |  1.54948\n",
            "----------------------------------------\n",
            "INFO - Step 652, rl-loss: 1706.7177734375\n",
            "----------------------------------------\n",
            "  episode      |  400\n",
            "  reward       |  1.82066\n",
            "----------------------------------------\n",
            "INFO - Step 818, rl-loss: 759.9742431640625\n",
            "----------------------------------------\n",
            "  episode      |  500\n",
            "  reward       |  1.85346\n",
            "----------------------------------------\n",
            "INFO - Step 975, rl-loss: 1890.02392578125\n",
            "----------------------------------------\n",
            "  episode      |  600\n",
            "  reward       |  1.93\n",
            "----------------------------------------\n",
            "INFO - Step 1100, rl-loss: 1210.627197265625\n",
            "INFO - Copied model parameters to target network.\n",
            "INFO - Step 1128, rl-loss: 1799.3046875\n",
            "----------------------------------------\n",
            "  episode      |  700\n",
            "  reward       |  1.41024\n",
            "----------------------------------------\n",
            "INFO - Step 1269, rl-loss: 1204.6671142578125\n",
            "----------------------------------------\n",
            "  episode      |  800\n",
            "  reward       |  1.1819\n",
            "----------------------------------------\n",
            "INFO - Step 1420, rl-loss: 1633.61767578125\n",
            "----------------------------------------\n",
            "  episode      |  900\n",
            "  reward       |  1.68262\n",
            "----------------------------------------\n",
            "INFO - Step 1587, rl-loss: 1735.5482177734375\n",
            "Logs saved in experiments/no_limit_holdem_dqn_result/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-aaa62e3ad50c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mDQNtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-44813ff183d9>\u001b[0m in \u001b[0;36mDQNtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 logger.log_performance(\n\u001b[1;32m     70\u001b[0m                     \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     tournament(\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_eval_games\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rlcard/utils/utils.py\u001b[0m in \u001b[0;36mtournament\u001b[0;34m(env, num)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_payoffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_payoffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_p\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_payoffs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rlcard/envs/env.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    135\u001b[0m         '''\n\u001b[1;32m    136\u001b[0m         \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Loop to play the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rlcard/envs/env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mbegining\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         '''\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_recorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rlcard/games/nolimitholdem/game.py\u001b[0m in \u001b[0;36minit_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Initialize players to play the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_chips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Initialize a judger class which will decide who wins in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rlcard/games/nolimitholdem/game.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Initialize players to play the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_chips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Initialize a judger class which will decide who wins in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"DQN example in RLCard\")\n",
        "    parser.add_argument(\n",
        "        '--env',\n",
        "        type=str,\n",
        "        default='no-limit-holdem',\n",
        "        choices=[\n",
        "            'no-limit-holdem',\n",
        "        ],\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--algorithm',\n",
        "        type=str,\n",
        "        default='dqn',\n",
        "        choices=[\n",
        "            'dqn',\n",
        "        ],\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--cuda',\n",
        "        type=str,\n",
        "        default='',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--seed',\n",
        "        type=int,\n",
        "        default=42,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--num_episodes',\n",
        "        type=int,\n",
        "        default=2000,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--num_eval_games',\n",
        "        type=int,\n",
        "        default=5000,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--evaluate_every',\n",
        "        type=int,\n",
        "        default=100,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--log_dir',\n",
        "        type=str,\n",
        "        default='experiments/no_limit_holdem_dqn_result/',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--save_path',\n",
        "        type=str,\n",
        "        default=os.path.join('experiments/no_limit_holdem_dqn_result/', 'model.pth'),\n",
        "    )\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda\n",
        "    DQNtrain(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbDsmuFjxumh"
      },
      "source": [
        "##Anything else that we need to include TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
